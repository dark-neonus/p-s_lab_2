---
title: 'P&S-2022: Lab assignment 2'
author: "Dominika Petrus, Nazar Pasichnyk, Daryna Onopriienko"
output:
  html_document:
    df_print: paged
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

```{r}
set.seed(12)
```

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
# your team id number 
                          ###
id <- 12                  ### Change to the correct id!
                          ###
set.seed(id)
p <- id/100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
# cat("The matrix G is: \n") 
#G  
#cat("The matrix H is: \n") 
#H
#cat("The product GH must be zero: \n")
#(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages

message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(100)
codewords <- (messages %*% G) %% 2

encode <- function(messages) {
  (messages %*% G) %% 2
}

add_noise <- function(codewords, p) {
  E <- matrix(rbinom(length(codewords), size=1, prob=p), nrow = nrow(codewords))
  (codewords + E) %% 2 
} 

syndrome_vec <- function(r_row) {
  as.integer((r_row %*% H) %% 2)
} 

correct_errored <- function(r_row) {
  z <- syndrome_vec(r_row)
  pos <- z[1] + 2*z[2] + 4*z[3]
  if (pos > 0)
    r_row[pos] <- (r_row[pos] + 1) %% 2
  r_row
}


decode_message <- function(r_star) {
  c(r_star[3], r_star[5], r_star[6], r_star[7])
}
```

Part 1. Simulating the encoding-transmission-decoding process and estimate $\hat p$

```{r}
simulate_hamming <- function(N, p) {
  messages  <- message_generator(N)
  codewords <- encode(messages)
  received  <- add_noise(codewords, p)
  corrected <- t(apply(received, 1, correct_errored))
  decoded   <- t(apply(corrected, 1, decode_message))
  
  success <- apply(decoded == messages, 1, all)
  rate <- mean(success)
  
  cat("estimated p =", rate, "\n")
  
  list(
    rate = rate,
    success = success,
    messages = messages,
    codewords = codewords,
    received = received,
    corrected = corrected,
    decoded = decoded
  )
}

N <- 10000
sim <- simulate_hamming(N, p)
p_hat <- sim$rate
p_hat

```

LLN (Law of Large Numbers) gaurantees that $\hat p$ -\> $p^*$ for large $N$.

Part 2. predict the confidence interval $(p^*-\varepsilon, p^* + \varepsilon)$,
```{r}

standard_error <- sqrt(p_hat*(1-p_hat)/N)
z <- qnorm(0.975)
epsilon <- z*standard_error

interval <- c(lower= p_hat - epsilon, upper= p_hat + epsilon)
interval

```
According to CLT:
$$
\frac{\hat{p} - p^*}{\sqrt{\dfrac{p^*(1 - p^*)}{N}}}
\;\approx\;
\mathcal{N}(0,1)
$$

then
$$
(p^* - \varepsilon,\; p^* + \varepsilon)
= \hat{p} \pm 1.96 \sqrt{\frac{\hat{p}(1 - \hat{p})}{N}}
$$


Part 3. What choice of $N$ guarantees that $\varepsilon \le 0.03$?
```{r}
eps <- 0.03
cat(p_hat)

N_worst <- ceiling((z^2)*0.25/(eps^2))
N_worst

```
$$
N = \frac{z^2 \cdot 0.25}{\varepsilon^2}
$$

Part 4. histogram
```{r}
K <- rbinom(N, size = 4, prob = p)

hist(K,
     breaks = seq(-0.5, 4.5, by = 1),
     col = "skyblue", border = "white",
     main = "Number of errors per 4-bit message",
     xlab = "k (number of bit errors)",
     ylab = "Frequency")
```

$$
K \sim {Bin}(4, p)
$$

**Conclusion**

Simulation showed that [7, 4] Hamming code works well for correcting single bit errors. Estimated p of successful transmission is around $\hat{p} \approx 0.88$, to guarantee that $\varepsilon \le 0.03$ $N \approx 1000$ was enough. The histogram of number of errored bits is a Binomial distibution with n=4, p=p. Everything behave as expected.

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

#### **Part 1: Calculate the Poisson parameter μ**

```{r}
# Physical constants and parameters for Cesium-137
# Half-life period in years
T_years <- 30.1
# Convert to seconds
T_seconds <- T_years * 365.25 * 24 * 3600
# Atomic mass of Cesium-137 (g/mol)
M <- 137
# Avogadro constant (atoms/mol)
N_A <- 6e23
# Mass of each sample in grams (id * 10^-6 g)
m <- id * 1e-6

# Calculate activity lambda: probability that one nucleus decays in one second
lambda <- log(2) / T_seconds

# Calculate number of atoms in one sample
N_atoms <- (m / M) * N_A

# Calculate Poisson parameter μ for one sample
mu <- N_atoms * lambda

cat("Poisson parameter μ for one sample:", mu, "\n")
cat("This represents the expected number of decays per second in one sample.\n")
```

#### **Part 2: Demonstrate convergence to normal distribution using CLT**

The Central Limit Theorem states that for i.i.d. random variables $X_1, X_2, \ldots, X_n$ with mean $\mu$ and variance $\sigma^2$, the sample mean $\bar{X}_n$ converges in distribution to $\mathscr{N}(\mu, \sigma^2/n)$ as $n \to \infty$.

For Poisson($\mu$), we have $\mathbb{E}[X_i] = \mu$ and $\text{Var}[X_i] = \mu$. Therefore, the sample mean should converge to $\mathscr{N}(\mu, \mu/n)$.

#### **Case n = 5**

```{r}
# Number of simulations
K <- 1e3
# Sample size
n <- 5

# Simulate K sample means, each computed from n Poisson observations
sample_means_5 <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))

# According to CLT, sample mean ~ N(mu, mu/n)
# Mean of the normal approximation
mu_normal_5 <- mu
# Standard deviation of the normal approximation
sigma_normal_5 <- sqrt(mu/n)

# Plot ecdf vs cdf
xlims_5 <- c(mu_normal_5 - 3*sigma_normal_5, mu_normal_5 + 3*sigma_normal_5)
Fs_5 <- ecdf(sample_means_5)

plot(Fs_5, 
     xlim = xlims_5, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = paste("ECDF vs CDF for n =", n))
curve(pnorm(x, mean = mu_normal_5, sd = sigma_normal_5), 
      col = "red", lwd = 2, add = TRUE)
legend("bottomright", 
       legend = c("Empirical CDF", "Normal CDF"), 
       col = c("blue", "red"), 
       lwd = 2)

# Calculate maximal difference (Kolmogorov-Smirnov statistic)
# https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test
x_vals_5 <- sort(sample_means_5)
empirical_cdf_5 <- Fs_5(x_vals_5)
theoretical_cdf_5 <- pnorm(x_vals_5, mean = mu_normal_5, sd = sigma_normal_5)
max_diff_5 <- max(abs(empirical_cdf_5 - theoretical_cdf_5))

cat("For n =", n, ":\n")
cat("  Theoretical mean μ =", mu_normal_5, "\n")
cat("  Theoretical standard deviation σ =", sigma_normal_5, "\n")
cat("  Sample mean of sample means =", mean(sample_means_5), "\n")
cat("  Sample std dev of sample means =", sd(sample_means_5), "\n")
cat("  Maximum difference between ECDFs =", max_diff_5, "\n\n")
```

#### **Case n = 10**

```{r}
# Sample size
n <- 10

# Simulate K sample means
sample_means_10 <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))

# Normal approximation parameters
mu_normal_10 <- mu
sigma_normal_10 <- sqrt(mu/n)

# Plot ecdf vs cdf
xlims_10 <- c(mu_normal_10 - 3*sigma_normal_10, mu_normal_10 + 3*sigma_normal_10)
Fs_10 <- ecdf(sample_means_10)

plot(Fs_10, 
     xlim = xlims_10, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = paste("ECDF vs CDF for n =", n))
curve(pnorm(x, mean = mu_normal_10, sd = sigma_normal_10), 
      col = "red", lwd = 2, add = TRUE)
legend("bottomright", 
       legend = c("Empirical CDF", "Normal CDF"), 
       col = c("blue", "red"), 
       lwd = 2)

# Calculate maximal difference
x_vals_10 <- sort(sample_means_10)
empirical_cdf_10 <- Fs_10(x_vals_10)
theoretical_cdf_10 <- pnorm(x_vals_10, mean = mu_normal_10, sd = sigma_normal_10)
max_diff_10 <- max(abs(empirical_cdf_10 - theoretical_cdf_10))

cat("For n =", n, ":\n")
cat("  Theoretical mean μ =", mu_normal_10, "\n")
cat("  Theoretical standard deviation σ =", sigma_normal_10, "\n")
cat("  Sample mean of sample means =", mean(sample_means_10), "\n")
cat("  Sample std dev of sample means =", sd(sample_means_10), "\n")
cat("  Maximum difference between ECDFs =", max_diff_10, "\n\n")
```

#### **Case n = 50**

```{r}
# Sample size
n <- 50

# Simulate K sample means
sample_means_50 <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))

# Normal approximation parameters
mu_normal_50 <- mu
sigma_normal_50 <- sqrt(mu/n)

# Plot ecdf vs cdf
xlims_50 <- c(mu_normal_50 - 3*sigma_normal_50, mu_normal_50 + 3*sigma_normal_50)
Fs_50 <- ecdf(sample_means_50)

plot(Fs_50, 
     xlim = xlims_50, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = paste("ECDF vs CDF for n =", n))
curve(pnorm(x, mean = mu_normal_50, sd = sigma_normal_50), 
      col = "red", lwd = 2, add = TRUE)
legend("bottomright", 
       legend = c("Empirical CDF", "Normal CDF"), 
       col = c("blue", "red"), 
       lwd = 2)

# Calculate maximal difference
x_vals_50 <- sort(sample_means_50)
empirical_cdf_50 <- Fs_50(x_vals_50)
theoretical_cdf_50 <- pnorm(x_vals_50, mean = mu_normal_50, sd = sigma_normal_50)
max_diff_50 <- max(abs(empirical_cdf_50 - theoretical_cdf_50))

cat("For n =", n, ":\n")
cat("  Theoretical mean μ =", mu_normal_50, "\n")
cat("  Theoretical standard deviation σ =", sigma_normal_50, "\n")
cat("  Sample mean of sample means =", mean(sample_means_50), "\n")
cat("  Sample std dev of sample means =", sd(sample_means_50), "\n")
cat("  Maximum difference between ECDFs =", max_diff_50, "\n\n")
```

#### **Comparison of results for different n**

```{r}
# Summary comparison
cat("Summary of CLT convergence:\n")
cat("n = 5:  Max difference =", max_diff_5, "\n")
cat("n = 10: Max difference =", max_diff_10, "\n")
cat("n = 50: Max difference =", max_diff_50, "\n\n")
cat("As n increases, the maximum difference decreases, confirming the CLT.\n")
cat("The empirical distribution converges to the theoretical normal distribution.\n")
```

#### **Part 3: Finding maximum number of samples with safety constraint**

We need to find the largest $n$ such that $P(\sum_{i=1}^n X_i < 8 \times 10^8) \geq 0.95$.

Let $S_n = \sum_{i=1}^n X_i$. Since $X_i \sim \text{Poisson}(\mu)$, we have $S_n \sim \text{Poisson}(n\mu)$ with $\mathbb{E}[S_n] = n\mu$ and $\text{Var}[S_n] = n\mu$.

```{r}
# Critical threshold for total decays
critical_value <- 8e8

# Method 1: Markov Inequality
# P(S_n >= c) <= E[S_n] / c
# P(S_n < c) >= 1 - E[S_n] / c
# We need: 1 - n*mu / c >= 0.95
# => n*mu / c <= 0.05
# => n <= 0.05 * c / mu

n_markov <- floor(0.05 * critical_value / mu)
cat("Markov Inequality bound: n <=", n_markov, "\n")

# Method 2: Chernoff Bound
# For Poisson(lambda), MGF: M(t) = exp(lambda(e^t - 1))
# P(S_n >= c) <= exp(-t*c) * E[exp(t*S_n)] = exp(-t*c + n*mu*(e^t - 1))
# Minimize over t > 0
# Optimal t: c = n*mu*e^t => t = log(c/(n*mu))
# For given n, bound is: exp(n*mu - c + c*log(c/(n*mu)))
# We need this <= 0.05, solve numerically

# Try different values of n to find the bound
chernoff_bound <- function(n, mu, c) {
  # Bound is >= 1, not useful
  if (n * mu >= c) return(1)
  exp(n * mu - c + c * log(c / (n * mu)))
}

# Binary search for maximum n
n_low <- 1
n_high <- n_markov * 10
while (n_high - n_low > 1) {
  n_mid <- floor((n_low + n_high) / 2)
  if (chernoff_bound(n_mid, mu, critical_value) <= 0.05) {
    n_low <- n_mid
  } else {
    n_high <- n_mid
  }
}
n_chernoff <- n_low
cat("Chernoff Bound: n <=", n_chernoff, "\n")

# Method 3: Central Limit Theorem
# S_n ~ N(n*mu, n*mu) approximately
# P(S_n < c) = P((S_n - n*mu)/sqrt(n*mu) < (c - n*mu)/sqrt(n*mu))
# Using standard normal: P(Z < z) = 0.95 => z ≈ 1.645
# (c - n*mu)/sqrt(n*mu) = 1.645
# c - n*mu = 1.645 * sqrt(n*mu)
# Let x = sqrt(n*mu), then: c - x^2 = 1.645*x
# x^2 + 1.645*x - c = 0
# x = (-1.645 + sqrt(1.645^2 + 4*c)) / 2
# n = x^2 / mu

z_95 <- qnorm(0.95)
discriminant <- z_95^2 + 4 * critical_value
x_solution <- (-z_95 + sqrt(discriminant)) / 2
n_clt <- floor(x_solution^2 / mu)
cat("CLT bound: n <=", n_clt, "\n\n")

# Compare the three methods
cat("Comparison of theoretical bounds:\n")
cat("  Markov:   n <=", n_markov, "(most conservative)\n")
cat("  Chernoff: n <=", n_chernoff, "\n")
cat("  CLT:      n <=", n_clt, "(least conservative)\n\n")
```

#### **Simulation to verify the CLT prediction**

```{r}
# Use the CLT prediction as it's typically the most accurate
n_predicted <- n_clt
# Number of simulations for verification
K_sim <- 1e4

cat("Verifying with n =", n_predicted, "samples\n")
cat("Simulating", K_sim, "realizations...\n\n")

# Simulate K_sim realizations of the sum S_n
sums_sample <- replicate(K_sim, sum(rpois(n_predicted, lambda = mu)))

# Calculate empirical probability
empirical_prob <- mean(sums_sample < critical_value)

cat("Results of simulation:\n")
cat("  Expected value E[S_n] = n*μ =", n_predicted * mu, "\n")
cat("  Sample mean of sums =", mean(sums_sample), "\n")
cat("  Sample std dev of sums =", sd(sums_sample), "\n")
cat("  Theoretical std dev =", sqrt(n_predicted * mu), "\n\n")

cat("  Critical value:", critical_value, "\n")
cat("  Empirical probability P(S_n <", critical_value, ") =", empirical_prob, "\n")
cat("  Desired probability: 0.95\n\n")

if (empirical_prob >= 0.95) {
  cat("SUCCESS: The empirical probability", empirical_prob, 
      "is at or above the desired 0.95 level.\n")
  cat("The laboratory can safely store up to", n_predicted, "samples.\n")
} else {
  cat("WARNING: The empirical probability", empirical_prob, 
      "is below the desired 0.95 level.\n")
  cat("Consider using a more conservative bound (n =", n_chernoff, ").\n")
}

# Histogram of the sums
hist(sums_sample, 
     breaks = 50, 
     probability = TRUE,
     main = paste("Distribution of S_n for n =", n_predicted),
     xlab = "Total number of decays",
     col = "lightblue",
     border = "white")

# Add normal approximation curve
curve(dnorm(x, mean = n_predicted * mu, sd = sqrt(n_predicted * mu)), 
      col = "red", lwd = 2, add = TRUE)

# Add vertical line at critical value
abline(v = critical_value, col = "darkgreen", lwd = 2, lty = 2)
legend("topright", 
       legend = c("Simulated", "Normal approx.", "Critical value"), 
       col = c("lightblue", "red", "darkgreen"), 
       lwd = c(10, 2, 2),
       lty = c(1, 1, 2))
```

#### **Conclusions for Task 2**

**Part 1:** We calculated the Poisson parameter $\mu$ for the radioactive decay of Cesium-137 samples. Using the half-life period, atomic mass, and sample mass, we determined the expected number of decays per second for one sample.

**Part 2:** We demonstrated the Central Limit Theorem by showing that sample means converge to a normal distribution as sample size increases:
- For $n = 5$, we observe some deviation from normality
- For $n = 10$, the fit improves significantly  
- For $n = 50$, the empirical CDF closely matches the theoretical normal CDF

The maximum difference between empirical and theoretical CDFs decreases as $n$ increases, confirming CLT convergence. This validates that even though individual decay counts follow a Poisson distribution, their averages are well-approximated by a normal distribution.

**Part 3:** We found the maximum number of samples that can be safely stored:
- **Markov inequality** provides the most conservative (smallest) bound but is often too pessimistic
- **Chernoff bound** gives a tighter bound by using moment generating functions
- **CLT** provides the least conservative bound and is most accurate for large sample sizes

Our simulation verified that the CLT prediction achieves approximately the desired 0.95 probability level, making it the most practical choice. The Chernoff bound serves as a good safety margin if needed. The normal approximation to the Poisson sum works well here because $n\mu$ is large enough for the CLT to apply effectively.

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

#### First, generate samples an sample means:

```{r}
teamid <- 12
v <- teamid + 10
nu1 <- 1/v  #µ = 1/lambda
sigma_squared <- 1/(v^2)
K <- 10000

for (n in c(5,10,50)){

  sample_means <- colMeans(matrix(rexp(n*K, rate = v), nrow=n)) #generate vctor of samples
  Fs <- ecdf(sample_means) #emprirical ecdf from sample
  
  mu <- nu1       # expectation of sample mean is just µ
  sigma <- sqrt(sigma_squared) / sqrt(n)   # sd
  
  cat("Parameters of the standard normal approximation", mu, sigma, "\n")
  
  # Define xlims for the plot
  xlims <- c(mu - 3*sigma, mu + 3*sigma)
  
  Fs <- ecdf(sample_means)
  plot(Fs, 
       xlim = xlims, 
       col = "pink",
       lwd = 2,
       main = "Comparison of ecdf and cdf")
  curve(pnorm(x, mean = mu, sd = sigma), col = "green", add = TRUE)
  
  # take a grid of 200 points tj within 3σ of µ
  t_lims <- c(mu - 3 * sigma,mu + 3 * sigma )
  
  t_points <- seq(t_lims[1], t_lims[2], length.out = 200)
  #count values of Fs_t, Fz_t 
  Fs_t <- Fs(t_points)
  Fz_t <- pnorm(t_points, mean = mu, sd = sigma)
  
  max_difference <- max(abs(Fs_t - Fz_t))
  
  cat("Max difference", max_difference, "\n")
  cat("We can see that with larger n (5,10,50) we have better approximation, max diference is less with each iteration,and graffics are almost overlap\n")
}
```

```{r}
#Part 2

#(a) express the event of interest in terms of the r.v. S := X1 + · · · + X100;

# We need to find P(S>60) ≥ 0.95
# Because if S is greater then 60 the 100's click happened after 1 minute. So , during one minute ≤ 99 clicks happened 
# Each Xk has exponential distribution with rate v, and v = N * v1(rate for distribution when   N=1) = 22N


#(b) obtain the theoretical bounds on Central Limit Theorem

lambda <- 22 * N
mu <- 1/lambda      # is equal to expectation of Xk
sd_Xk <- 1/lambda  # because standart deviation is square root from variance

# we will use standartized sum and formula Zn = (S - mu*n) / sd_Xk * √n
# mu is 1/lambda and sigma is standart deviation, also 1/lambda and √n is equal to √100 = 10
```

(a) P(S\>60) ≥ 0.95

(b) at first let's calculate first part:\
    P (S - 100/lambda)/10/lambda \> (60 - 100/lambda)/(10/lambda)\
    \
    P(Z \> (60 - 100/lambda)/(10/lambda)) = P(Z \> (60 - 100/22N)/(10/22N)) =\
    \
    = P(Z \> (60\*22N - 100)/10) = P(Z \> (6\*22N - 10)) = P(Z \> (132\*N - 10)) =\
    \
    = 1 - P(Z ≤ (132\*N - 10))

(c) P(S\>60) ≥ 0.95\
    1 - P(Z ≤ (132\*N - 10)) ≥ 0.95\
    1 - P(Z ≤ (132\*N - 10)) ≥ 0.95\
    P(Z ≤ (132\*N - 10)) ≤ 0.05\
    Ф(132\*N - 10) ≤ Ф(0.05)\
    132\*N - 10 ≤ -1.645 (from the table)\
    132\*N ≤ 8.335\
    N ≤ 8.335/132\
    N ≤ 0.063

```
{r}
N_CLT <- floor(8.335/132)
```
```
{r}

# MARKOV INEQUALITY P(X ≥ a) ≤ E[X]​/a

#Let's assume that D is the number of clicks within one minute. And clicks per minute must not exceed 100
# E(D) = 60 * lambda (where lambda is amount of clicks in second)
# P(D≥101) ≤ E(D)/101 and we need probability of unsafe event not exceed 0.05 so we also have boundary 
# P(D≥101) ≤ 0.05 so from this :E(D)/101 ≤ 0.05 
# 60 * lambda / 101 ≤ 0.05 from that -> lambda ≤ 0.05 * 101 / 60 -> lambda ≤ 0.0841
# and lambda is equal to 22*N , we khow that  N ≤  0.0841 / 22 
# N ≤ 0.0038

team_id <- 22
prob_unsafe <- 0.05 # 1-0.95
time <- 60
lambda <- (prob_unsafe* 101) / 60
N_markov <- lambda / team_id
cat("Results from Markov:", N_markov)
```

```
{r}
# For c and d we use N = 1, because previous result shows that N is equal 0, it means that different N will give not correct results, so in simulation we use 1 (the closest non-negative integer to 0 is 1)  and show that it will give a result that violates the safety condition


#simulate the realization... of the  S = X1 + ... + X100

v1 <- 22
n <- 100
N <- 1
v <- v1 * N
K <- 1000  # amount of simulation repiting

# part c
x_realizations <- rexp(n, rate = v)  #r{distr} generates a sample
S_for_100 <- sum(x_realizations)

#part d :  repeat this K times to get the sample s = (s1, . . . , sK) of total times until the 100th click

#generate matrix with samples and calculate the 
S <- matrix(rexp(n * K, rate = v), nrow = n)
S_total_time <- colSums(S)

time_boundary <- 60
prob <- mean(S_total_time > time_boundary)

cat("Result for c:", S_for_100, "\n",
    "Result for d", prob, "\n", " Probability equals to 0 means that in 10,000 simulations, at no time 100 clicks take more than 60 seconds")
```

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

1.  **In this part, we discuss independence of random variables and its moments: expectation and variance.**

    1.  Suppose we have a random variable $X$. Explain why $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

    2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of $Y := \frac{1}{X}$ to calculate the values of $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$. Comment on the received results;

        ```{r}
        mu <- 12 # team id
        sigma <- 31 # 2 * team id number + 7
        sd = sqrt(sigma)
        N <- 100


        x = rnorm(N, mean = mu, sd = sd) # Simulate realization for X
        y = 1/x # realization for Y

        mean_x <- mean(x)  # X̄
        mean_y <- mean(y)  # Ȳ
        inverted_mean_x <- 1 / mean_x

        #show results
        print("Sample mean X̄ " )
        print(mean_x)

        print("Inverted mean 1/X")
        print(inverted_mean_x)

        print("Sample mean Ȳ = mean(1/X)")
        print(mean_y)

        print("Difference between inverted mean [1/X̄] and  sample mean Ȳ [mean(1/X)] ")

        print(abs(inverted_mean_x - mean_y))

        print("We can see that difference is not equal to zero, so the equality 1/X̄ = [mean(1/X)] is not true")

        ```

    3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?

        ```{r}
        lambda <- 2
        n <- 100

        #generate a cdf for X and Y and set Z
        X <- rexp(n, rate = 2)
        Y <- rexp(n, rate = 2)
        Z = log(X) + 5  # Z is dependent on X

        # Q-Q plot for X and Y
        qqplot(X, Y, main = "Q-Q plot for X and Y",
            xlab = "X quantiles",
            ylab = "Y quantiles",
            col = "pink")
        abline(0, 1, col = "blue")

        # scatterplot for X and Y
        plot(X, Y, main = "scatterplot for X and Y",
            xlab = "X",
            ylab = "Y",
            col = "pink")

        # Q-Q plot for X and Z
        qqplot(X, Z, main = "Q-Q Plot for X and Z",
            xlab = "X quantiles",
            ylab = "Z quantiles",
            col = "purple")
        abline(0, 1, col = "blue")
        
        # scatterplot for X and Z
        plot(X, Z, main  = "scatterplot for X and Z",
            xlab = "X",
            ylab = "Z",
            col = "purple")

        cat("Summary:

        For X and Y in Q-Q plot :
        
        X and Y have same distribution ~ exp(2), so the quantiles coincide
        that form a plot where points overlap,
        and align along the line y = x
        
        For X and Y in scatterplot :
        
        It looks like graph with random points
        because there is no functional relationship between X and Y

        For X and Z in Q-Q plot :
        
        X has an exponential distribution when Z not
        
        For X and Y in scatterplot :
        
        We can saw that they are dependent and as result 
        points form a logarithmic curve
        
    Which pair of r.v.'s is dependent and which one is similar?
        
        X and Z are dependent and X and Y - similar")
        ```

    ------------------------------------------------------------------------

2.  You toss a fair coin three times and a random variable $X$ records how many times the coin shows Heads. You convince your friend that they should play a game with the following payoff: every round (equivalent to three coin tosses) will cost £$1$. They will receive £$0.5$ for every coin showing Heads. What is the expected value and the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

    2.  What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;

        ```{r}
        # Expected value if Binomial r.v. = n*p
        # Variance of Binomial r.v. = n*p*(1-p)
        n <- 3
        p <- 1/2
        expected_value_X <- n*p
        variance_X <- n*p*(1-p)
        N <- 100  # amoumt of values for simulation


        sample_for_simulation_X <- rbinom(N, size = n, prob = p) #generate sample for X 

        sample_mean_X <- mean(sample_for_simulation_X)

        squared_deviations_X <- (sample_for_simulation_X - sample_mean_X)^2
        sample_variance_X <- sum(squared_deviations_X) / (N- 1)


        cat("Results:
            Expected value of X",expected_value_X,
            "Variance of X" , variance_X,
        
            "Sample mean of X", sample_mean_X,
            "Sample variance of X", sample_variance_X)
        ```

    3.  What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;

        ```{r}
        #Y = 0.5 *X - 1
        # to calculate expected value of Y we apply linearity of expectation
        # E(Y) = E(0.5*X - 1) =  E(0.5*X) - E(1) = 0.5 * E(X) - 1

        expected_value_Y <- (0.5 * expected_value_X) - 1

        # and to calculate variance of Y we also apply its properties
        # Var(0.5*X - 1) = Var(0.5*X) = 0.5^2*Var(X) = 0.25*Var(X)

        variance_Y <- 0.25 * variance_X

        #generate sample for Y
        sample_for_simulation_Y = 0.5 * sample_for_simulation_X -1
        sample_mean_Y <- mean(sample_for_simulation_Y)

        squared_deviations_Y <- (sample_for_simulation_Y - sample_mean_Y)^2
        sample_variance_Y <- sum(squared_deviations_Y) / (N- 1)

        cat("Results:
            Expected value of Y",expected_value_Y,
            "Variance of Y" , variance_Y,
        
            "Sample mean of Y", sample_mean_Y,
            "Sample variance of Y", sample_variance_Y)
        ```

------------------------------------------------------------------------

### General summary and conclusions

Summarize here what you've done, whether you solved the tasks, what difficulties you had etc.
